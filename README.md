<p align="center">
  <img src=".well-known/logo-nobg.png" alt="Pira" width="40%" />
</p>

# Pyramind: Hierarchical Memory Recall System for Language Models

This repository contains the implementation of a Hierarchical Memory Recall System for Language Models (LLMs), such as ChatGPT. The aim of this project is to enhance the memory recall capabilities of LLMs by organizing chat histories into a hierarchical structure with varying levels of abstraction.

## Usage

With the Pyramind plugin enabled, ChatGPT will try to proactively remember dialogues it considers important, but of course, you can also inform it to remember things you want it to. The Pyramind plugin organizes chat histories into a hierarchical structure with varying levels of abstraction, allowing ChatGPT to retrieve information at different levels of abstraction based on user queries, enabling access to both specific and abstract information.

## Quickstart (largely from [chatgpt-retrieval-plugin](https://github.com/openai/chatgpt-retrieval-plugin))

Follow these steps to quickly set up and run the ChatGPT Retrieval Plugin:

1. Install Python 3.10, if not already installed.
2. Clone the repository: `git clone https://github.com/leo4life2/hierarchical_memory_plugin.git`
3. Navigate to the cloned repository directory: `cd /path/to/hierarchical_memory_plugin`
4. Install poetry: `pip install poetry`
5. Create a new virtual environment with Python 3.10: `poetry env use python3.10`
6. Activate the virtual environment: `poetry shell`
7. Install app dependencies: `poetry install`
8. Rename `.env.example` to `.env`, and set the required environment variables (make sure you have a Pinecone account and index created).
9. Run `source .env` to load the environment variables.
10. Run the API locally: `poetry run start`
11. Access the API documentation at `http://0.0.0.0:8000/docs` and test the API endpoints (make sure to add your bearer token).

## Terminology and Concepts

Before diving into the details of the implementation, it is essential to understand some terminology and concepts used in this project.

### Memory Chunks

Memory chunks are the smallest units of chat history in our hierarchical memory recall system. They are fixed-sized segments of 200 tokens created from the chat history. We summarize every 5 consecutive memory chunks at any level to create higher-level, more abstract representations, enabling the system to provide meaningful context and responses even when user prompts involve abstract ideas or concepts with low vector similarity to specific chat histories.

### Hierarchical Memory Structure

The hierarchical memory structure is a way of organizing memory chunks and their summaries at multiple levels of abstraction. The levels are denoted as L1, L2, L3, etc., with L1 being the base level consisting of the original memory chunks.

- **L1 Memories**: These are the original memory chunks created from the chat history.
- **L2 Memories**: Summaries of every 5 consecutive L1 memory chunks, providing a more abstract representation of the information.
- **L3 Memories**: Summaries of every 5 consecutive L2 memory chunks, providing an even more abstract representation of the information.
- ...and so on, with higher levels summarizing the information from the level directly below.

By organizing the memory in this hierarchical structure, the system can optimize its ability to recall specific memory parts and abstract or big-picture memories, considering the limitations of vectorstores and retrieval.

### Importance of Summarized Memories for Abstract Concepts

When a user provides a prompt about an abstract idea or concept, it may have very low vector similarity with specific chat histories. In such cases, the summarized memories containing more abstract concepts (e.g., L2, L3, or higher-level memories) become crucial for providing meaningful context and responses.

The hierarchical memory structure enables the system to retrieve relevant information from different levels of abstraction, allowing it to generate responses that accurately address the user's abstract queries. This improves the overall quality and relevance of the responses generated by the language model.

## How it works

1. **Memory chunk creation**: Chat histories are divided into fixed-sized "memory chunks" of 200 tokens. The use of an overlapping window approach to preserve context is optional, and the length of the overlap is configurable.

2. **Hierarchical memory structure**: Summarize every 5 consecutive memory chunks of one level (e.g., L1 memories) into a more abstract representation called a higher-level memory (e.g., L2 memories). The process is repeated for higher levels, with 5 L2 memories summarized into an L3 memory, and so on.

3. **Utilizing ChatGPT API for summarization**: The ChatGPT API is used to create summaries for higher-level memories, with the LLM accessing the memory vectorstore to provide improved context during the summarization process.

4. **Storing memory chunks**: Each memory chunk is stored in a vectorstore without a specific structure, with metadata attached to each memory chunk, including its level number (e.g., L1, L2) and references to the lower-level chunks it represents.

5. **Maintaining all levels of memory chunks in the vectorstore**: The LLM retrieves information at different levels of abstraction based on user queries, enabling access to both specific and abstract information.

6. **Updating the hierarchical structure**: During chatting, the system checks if the latest 5 L1 memories are summarized or not. If not, they are summarized to create an L2 memory. This process continues for higher levels as necessary, always checking if 5 consecutive memories of a certain level are summarized before creating a higher-level memory.

7. **Retrieval process**: When a user provides a prompt, a similarity search is performed with the prompt to retrieve relevant memories from the vectorstore, regardless of their level of abstraction. The top result from vector similarity is assumed to provide the best context. If the context window allows for more tokens, additional relevant memories can be included to provide a more comprehensive response.

## Supported Vector Databases

This repo is largely based on the [chatgpt-retrieval-plugin](https://github.com/openai/chatgpt-retrieval-plugin) repo, and all the vector databases it supports should work here, but it currently has only been tested on the Pinecone vectorstore.
